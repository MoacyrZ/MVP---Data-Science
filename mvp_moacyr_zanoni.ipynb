{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoacyrZ/MVP---Data-Science/blob/main/mvp_moacyr_zanoni.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4IR1RF2XEr"
      },
      "source": [
        "# Algoritmo de Identificação de Guitarras  \n",
        "\n",
        "## 1. Introdução  \n",
        "\n",
        "O objetivo deste projeto é desenvolver um modelo de **deep learning** capaz de classificar imagens de guitarras elétricas e violões em seus respectivos modelos: **Stratocaster, Les Paul, Telecaster, SG, Super Strato, Semi Acústica, Flying V e Violão**. Trata-se, portanto, de um problema de **classificação multiclasse**, em que cada imagem deve ser associada corretamente a uma das oito categorias.  \n",
        "\n",
        "A aplicação desse tipo de solução pode trazer benefícios em diferentes contextos: **lojas de instrumentos musicais** que buscam organizar seu estoque automaticamente, **músicos** interessados em reconhecer modelos em coleções pessoais, e **aplicativos de marketplace de instrumentos usados**, que poderiam oferecer identificação automática de anúncios para melhorar a experiência do usuário.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Considerações  \n",
        "\n",
        "Como o projeto precisa ser reproduzido e avaliado pelos professores, foram adotados critérios para garantir **viabilidade prática**:  \n",
        "- O **dataset** não poderia ser excessivamente grande, evitando sobrecarga computacional.  \n",
        "- O processo de **treinamento e inferência** deveria ser executado em tempo razoável, mas ainda assim garantindo **desempenho competitivo**.  \n",
        "- Foi escolhida uma arquitetura **pré-treinada e eficiente (MobileNetV2)**, que equilibra velocidade de execução com boa capacidade de generalização.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Desafios  \n",
        "\n",
        "O problema apresenta algumas complexidades específicas:  \n",
        "- **Similaridade estética** entre certos modelos (ex.: Stratocaster e Super Strato, Les Paul e Violão).  \n",
        "- **Existência de variações dentro da mesma categoria**, como versões semi-acústicas de Les Paul e Telecaster.  \n",
        "- **Diversidade visual** em fotos reais, que podem ter fundos complexos, diferentes iluminações e ângulos variados.  \n",
        "\n",
        "Esses fatores aumentam a dificuldade do modelo, tornando necessária a aplicação de técnicas de **data augmentation, fine-tuning e avaliação com métricas robustas** como o **F1-score**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Instalação de Bibliotecas\n",
        "\n",
        "O primeiro bloco de código do notebook é responsável pela **instalação e importação das bibliotecas necessárias** para a execução do projeto. Essas bibliotecas incluem ferramentas para manipulação de dados, processamento de imagens, construção e treinamento de modelos de deep learning, além de funções auxiliares para visualização gráfica e avaliação de desempenho.  \n",
        "\n",
        "Em especial, destacam-se:  \n",
        "- **TensorFlow/Keras**: construção, treinamento e avaliação da rede neural convolucional baseada na MobileNetV2.  \n",
        "- **NumPy**: manipulação de arrays e operações matemáticas.  \n",
        "- **Matplotlib**: geração de gráficos e visualizações dos resultados.  \n",
        "- **scikit-learn**: cálculo de métricas de avaliação adicionais, como o F1-score.  \n",
        "\n",
        "Esse passo garante que o ambiente de execução esteja devidamente configurado e com todas as dependências instaladas para rodar o MVP de forma reprodutível.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE7OdwtXMq_7",
        "outputId": "d46c41b8-e33d-4ae6-8a4d-a6a4760826c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fcjx9Jyl6Ep-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import shutil\n",
        "import json\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Rescaling, RandomFlip, RandomRotation, RandomZoom,\n",
        "    Dense, GlobalAveragePooling2D, Dropout\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfEsU3XeNX9Z"
      },
      "source": [
        "## 3. Escolha do framework\n",
        "\n",
        "Após analisar em detalhes as bibliotecas **Keras** e **PyTorch**, optei por utilizar o **Keras** como framework principal.  \n",
        "A decisão foi baseada em fatores como **simplicidade**, **produtividade** e **velocidade de prototipagem**, já que o objetivo do projeto é testar diferentes técnicas de deep learning de forma ágil, com foco no aprendizado e na aplicação prática.  \n",
        "Embora o PyTorch ofereça maior flexibilidade e controle fino sobre o modelo, o Keras permite atingir resultados competitivos com menos complexidade de código, o que foi determinante para este MVP.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Escolha da arquitetura\n",
        "\n",
        "A escolha da arquitetura foi guiada pelo problema proposto: **classificação multiclasse de guitarras elétricas e violão**.  \n",
        "Inicialmente, defini que seria utilizado **Deep Learning**, pois além de ser uma área de interesse pessoal, o projeto também contribui para o fortalecimento do portfólio acadêmico e profissional.  \n",
        "Foram avaliadas algumas arquiteturas pré-treinadas disponíveis no Keras Applications, considerando principalmente o **equilíbrio entre custo computacional e desempenho**.\n",
        "\n",
        "### 4.1 MobileNetV2\n",
        "Selecionada por ser uma arquitetura **leve e eficiente**, projetada para rodar em dispositivos com recursos limitados.  \n",
        "Por sua rapidez e baixo consumo de memória, encaixa-se perfeitamente nos requisitos do projeto, mantendo boa capacidade de generalização.\n",
        "\n",
        "### 4.2 EfficientNetB0\n",
        "Considerada por seu **excelente trade-off entre acurácia e eficiência computacional**.  \n",
        "É uma arquitetura moderna, baseada em um escalonamento balanceado de profundidade, largura e resolução, oferecendo bom desempenho mesmo com custo computacional reduzido.\n",
        "\n",
        "### 4.3 InceptionV3\n",
        "Incluída para diversificação dos testes, de modo a comparar arquiteturas leves (MobileNetV2 e EfficientNetB0) com uma opção **mais robusta**.  \n",
        "Apesar de mais pesada, o InceptionV3 oferece um alto poder de extração de características e foi utilizado como referência para avaliar possíveis ganhos de desempenho em relação às arquiteturas otimizadas para eficiência.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXciL-hhVdYU"
      },
      "source": [
        "## 5. Construção e Evolução do Dataset  \n",
        "\n",
        "O dataset utilizado neste projeto foi desenvolvido a partir de diferentes fontes e passou por diversas versões até atingir um ponto considerado adequado para o treinamento do modelo.  \n",
        "A base inicial foi obtida no repositório público [Guitar Image Classification Dataset](https://images.cv/dataset/guitar-image-classification-dataset), posteriormente ampliada e refinada com imagens coletadas via *web scraping*.  \n",
        "\n",
        "As versões foram evoluindo da seguinte forma:  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.1 Dataset V1  \n",
        "Utilização direta do dataset original do link mencionado.  \n",
        "As imagens estavam organizadas em pastas correspondentes aos diferentes modelos de guitarras.  \n",
        "**Resultado:** o desempenho do modelo foi insatisfatório, evidenciando a necessidade de ampliar a base de dados.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.2 Dataset V2  \n",
        "Ampliação do dataset com imagens coletadas por *web scraping*, cobrindo diferentes modelos de guitarras.  \n",
        "**Resultado:** houve melhora em relação ao V1, mas ainda insuficiente para um bom nível de generalização.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.3 Dataset V3  \n",
        "Refinamento da curadoria das imagens, com remoção de:  \n",
        "- Fotos contendo mais de uma guitarra.  \n",
        "- Imagens de guitarras fotografadas de lado ou de costas.  \n",
        "- Imagens de baixa qualidade ou em que a guitarra não aparecia de forma clara.  \n",
        "**Resultado:** melhoria significativa nos resultados do modelo, mostrando a importância da qualidade dos dados.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.4 Dataset V4  \n",
        "Aumento direcionado do número de imagens em classes mais complexas, onde o modelo apresentava maior dificuldade, como:  \n",
        "- **Stratocaster vs. Super Strato**  \n",
        "- **Les Paul vs. Semi Acústica**  \n",
        "**Resultado:** maior equilíbrio entre classes e melhor desempenho nas categorias mais semelhantes entre si.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.5 Dataset Final  \n",
        "Com o dataset consolidado, foi realizado o **redimensionamento das imagens**, de modo a reduzir o espaço de armazenamento e padronizar a entrada do modelo (224x224 pixels), garantindo consistência durante o treinamento e a validação.  \n",
        "**Resultado:** dataset mais balanceado, limpo e adequado para experimentação com diferentes arquiteturas de deep learning.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20azK6s48Qkc",
        "outputId": "43d60fa9-1e0a-4f23-bc97-894b6ca8b188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extraído em: dataset\n",
            "Subpastas: ['sg', 'violao', 'semi_acustica', 'flying_v', 'telecaster', 'super_strato', 'les_paul', 'stratocaster']\n",
            "sg: 157 imagens\n",
            "violao: 140 imagens\n",
            "semi_acustica: 147 imagens\n",
            "flying_v: 139 imagens\n",
            "telecaster: 129 imagens\n",
            "super_strato: 137 imagens\n",
            "les_paul: 168 imagens\n",
            "stratocaster: 153 imagens\n"
          ]
        }
      ],
      "source": [
        "# URL do dataset zipado no GitHub (raw link!)\n",
        "url = \"https://raw.githubusercontent.com/MoacyrZ/MVP---Data-Science/main/dataset.zip\"\n",
        "zip_name = \"dataset.zip\"\n",
        "extract_dir = \"dataset\"\n",
        "\n",
        "# Se já existir dataset antigo, apagar\n",
        "if os.path.exists(extract_dir):\n",
        "    shutil.rmtree(extract_dir)\n",
        "\n",
        "if os.path.exists(zip_name):\n",
        "    os.remove(zip_name)\n",
        "\n",
        "# Baixar sempre a versão mais recente do GitHub\n",
        "r = requests.get(url)\n",
        "with open(zip_name, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "# Extrair\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Ajustar automaticamente se houver subpasta única (ex: \"1\")\n",
        "subpastas = [p for p in os.listdir(extract_dir) if os.path.isdir(os.path.join(extract_dir, p))]\n",
        "if len(subpastas) == 1:\n",
        "    DATASET_DIR = os.path.join(extract_dir, subpastas[0])\n",
        "else:\n",
        "    DATASET_DIR = extract_dir\n",
        "\n",
        "print(f\"Dataset extraído em: {DATASET_DIR}\")\n",
        "print(\"Subpastas:\", os.listdir(DATASET_DIR))\n",
        "\n",
        "# Mostrar quantidade de imagens por pasta\n",
        "for pasta in os.listdir(DATASET_DIR):\n",
        "    caminho = os.path.join(DATASET_DIR, pasta)\n",
        "    if os.path.isdir(caminho):\n",
        "        print(f\"{pasta}: {len(os.listdir(caminho))} imagens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StLbDAEAXRT3"
      },
      "source": [
        "## 6. Analisando o Dataset  \n",
        "\n",
        "Nesta etapa foi realizada uma contagem do número de imagens em cada categoria, a fim de avaliar se o dataset apresenta **balanceamento adequado** entre as classes.  \n",
        "A análise da distribuição é fundamental para entender possíveis **desequilíbrios**, que podem impactar o treinamento do modelo e gerar viés em algumas classes.  \n",
        "\n",
        "Além da contagem numérica, também foi gerado um gráfico de barras para facilitar a visualização das diferenças no número de exemplos por categoria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WT7cou2PKL6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad1d81c-bfca-455e-da54-e329a201bb94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1170 files belonging to 8 classes.\n",
            "Using 936 files for training.\n",
            "Found 1170 files belonging to 8 classes.\n",
            "Using 234 files for validation.\n",
            "Número de classes: 8\n",
            "Classes: ['flying_v', 'les_paul', 'semi_acustica', 'sg', 'stratocaster', 'super_strato', 'telecaster', 'violao']\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "\n",
        "# Dataset de treino\n",
        "train_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATASET_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=SEED,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "# Dataset de validação\n",
        "val_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATASET_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=SEED,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "# Classes\n",
        "class_names = train_ds_raw.class_names\n",
        "num_classes = len(class_names)\n",
        "print(f\"Número de classes: {num_classes}\")\n",
        "print(f\"Classes: {class_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9BkYLE8ZZBn"
      },
      "source": [
        "## 7. Transformação das Imagens e Data Augmentation  \n",
        "\n",
        "Todas as imagens foram padronizadas para o tamanho de **224x224 pixels**, garantindo compatibilidade com a arquitetura escolhida (MobileNetV2).  \n",
        "Na primeira tentativa, o redimensionamento foi feito de forma direta, o que causou **distorção nas proporções** das guitarras.  \n",
        "Após identificar esse problema, a abordagem foi modificada utilizando **`resize_with_pad`**, que adiciona tarjas pretas quando necessário, mantendo as proporções originais da imagem.  \n",
        "Esse ajuste resultou em uma melhoria no desempenho do modelo.  \n",
        "\n",
        "---\n",
        "\n",
        "### 7.1 Separação do Dataset de Treino  \n",
        "Foi realizada a divisão clássica **80% para treino** e **20% para validação**.  \n",
        "Apesar do dataset relativamente pequeno, optou-se por manter essa proporção para garantir volume suficiente de amostras para treino sem comprometer a validação.  \n",
        "\n",
        "---\n",
        "\n",
        "### 7.2 Separação do Dataset de Validação  \n",
        "Os **20% restantes** foram destinados exclusivamente à validação, garantindo que o modelo fosse avaliado com dados nunca vistos durante o treino.  \n",
        "\n",
        "---\n",
        "\n",
        "### 7.3 Data Augmentation  \n",
        "Como o dataset possui em média **150 imagens por categoria**, a técnica de **Data Augmentation** foi fundamental para aumentar a variabilidade dos exemplos de treino.  \n",
        "\n",
        "- **Treino**: aplicadas as seguintes transformações:  \n",
        "  - Normalização (escala 0–1).  \n",
        "  - **Random Flip** (espelhamento horizontal).  \n",
        "  - **Random Rotation** (rotações leves).  \n",
        "  - **Random Zoom** (ampliações e reduções parciais).  \n",
        "\n",
        "- **Validação**: aplicada apenas a **normalização**, evitando *data leakage* e assegurando avaliação justa do desempenho do modelo.  \n",
        "\n",
        "Essa estratégia contribuiu para uma **melhor generalização**, reduzindo o risco de *overfitting* e melhorando a robustez do modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8WeU3bGLULQ"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "data_augmentation = Sequential([\n",
        "    Rescaling(1./255),\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.1),\n",
        "    RandomZoom(0.1)\n",
        "])\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds_raw.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds_raw.map(lambda x, y: (Rescaling(1./255)(x), y))\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Salvar nomes das classes\n",
        "with open(\"class_names.json\", \"w\") as f:\n",
        "    json.dump(class_names, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L11VhNdYiVzI"
      },
      "source": [
        "## 8. Experimentos  \n",
        "\n",
        "O primeiro experimento consistiu em rodar a arquitetura **MobileNetV2** com a primeira versão do dataset, sem técnicas de pré-processamento ou refinamento. O resultado obtido foi de aproximadamente **12% de acurácia**, o que equivale a uma classificação aleatória.  \n",
        "\n",
        "Na segunda versão do dataset, com mais imagens e após 30 épocas de treino, o desempenho subiu para aproximadamente **50% de acurácia**, mostrando que o modelo já não fazia inferências totalmente aleatórias.  \n",
        "\n",
        "A seguir, foram testadas três arquiteturas diferentes:  \n",
        "- **MobileNetV2**  \n",
        "- **EfficientNetB0**  \n",
        "- **InceptionV3**  \n",
        "\n",
        "Os resultados indicaram que **MobileNetV2** e **InceptionV3** obtiveram desempenhos razoáveis (em torno de 50%), enquanto o **EfficientNetB0** ficou em apenas 15%. Dada a simplicidade, eficiência e compatibilidade com os recursos disponíveis, foi escolhida a **MobileNetV2** para dar continuidade ao projeto.  \n",
        "\n",
        "Com a arquitetura definida, foram aplicadas melhorias no dataset e na otimização com o **Adam**, elevando os resultados para aproximadamente **75% de acurácia**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9. Pipeline de Treinamento e Fine-Tuning  \n",
        "\n",
        "O pipeline de treinamento foi estruturado em diferentes etapas, utilizando **transfer learning** a partir de uma base pré-treinada no **ImageNet**. Essa abordagem permite aproveitar o conhecimento prévio da rede em detecção de padrões, formas e texturas, acelerando o processo de aprendizagem.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.1 Transfer Learning  \n",
        "Foi substituída a “cabeça” do modelo pré-treinado, adaptando a rede para classificar as **8 categorias de guitarras** do projeto.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.2 Congelamento da Base  \n",
        "Inicialmente, a base do modelo pré-treinado foi **congelada**, de modo a preservar o conhecimento adquirido no ImageNet e evitar que fosse sobrescrito logo nas primeiras épocas.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.3 Escolha da Métrica  \n",
        "Como se trata de um problema de **classificação multiclasse**, as métricas monitoradas foram:  \n",
        "- **Acurácia**, para medir a taxa de acertos globais.  \n",
        "- **F1-score (macro)**, para capturar o desempenho médio entre as classes e evitar que categorias mais frequentes dominassem a avaliação.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.4 Otimizador  \n",
        "Foram estudadas alternativas de otimizadores, mas o **Adam** apresentou melhor equilíbrio entre velocidade de convergência e desempenho final. Por ser adaptativo e eficiente, foi mantido como padrão nos experimentos.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.5 Lidando com Overfitting  \n",
        "Durante os experimentos iniciais, o modelo apresentou sinais de **overfitting**. Para lidar com isso, foram adotadas as seguintes estratégias:  \n",
        "- Ajuste da taxa de **Dropout**, reduzindo a complexidade da rede.  \n",
        "- Uso do **EarlyStopping**, interrompendo o treino quando não havia mais ganhos significativos.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.6 Fine-Tuning  \n",
        "Na etapa final, a base pré-treinada foi **parcialmente descongelada**, liberando apenas as últimas camadas para ajuste fino. Esse processo foi conduzido com uma **taxa de aprendizado reduzida**, permitindo que o modelo refinasse os pesos em função das guitarras, sem comprometer o conhecimento geral adquirido com o ImageNet.  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, val_ds):\n",
        "        super().__init__()\n",
        "        self.val_ds = val_ds\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_true, y_pred = [], []\n",
        "        for x, y in self.val_ds:\n",
        "            preds = self.model.predict(x, verbose=0)\n",
        "            y_true.extend(np.argmax(y.numpy(), axis=1))\n",
        "            y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} - Val Accuracy: {acc:.4f} - Val F1 (macro): {f1:.4f}\")\n",
        "        logs[\"val_accuracy_sklearn\"] = acc\n",
        "        logs[\"val_f1_macro\"] = f1\n"
      ],
      "metadata": {
        "id": "q0MDAqRLgPSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyqG2p7IMVD9"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 12\n",
        "INITIAL_EPOCH = 4\n",
        "LEARNING_RATE = 0.001\n",
        "FINE_TUNE_EPOCHS = 10\n",
        "\n",
        "UNITS_0, DROPOUT_0 = 128, 0.2\n",
        "UNITS_1, DROPOUT_1 = 448, 0.3\n",
        "\n",
        "# Base pré-treinada\n",
        "base_model = MobileNetV2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "base_model.trainable = False  # congelada inicialmente\n",
        "\n",
        "# Cabeça customizada\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(UNITS_0, activation=\"relu\")(x)\n",
        "x = Dropout(DROPOUT_0)(x)\n",
        "x = Dense(UNITS_1, activation=\"relu\")(x)\n",
        "x = Dropout(DROPOUT_1)(x)\n",
        "output = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compilação\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "metrics_callback = MetricsCallback(val_ds)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stop, metrics_callback]\n",
        ")\n"
      ],
      "metadata": {
        "id": "tR6w7Gkygkpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descongela parte da base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompilar\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "early_stop_ft = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
        "metrics_callback_ft = MetricsCallback(val_ds)\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=FINE_TUNE_EPOCHS,\n",
        "    callbacks=[early_stop_ft, metrics_callback_ft]\n",
        ")\n"
      ],
      "metadata": {
        "id": "szurU6pOgyov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQbdEMqy3l7h"
      },
      "source": [
        "## 10. Avaliação e Gráficos  \n",
        "\n",
        "Para avaliar o desempenho do modelo, foram gerados gráficos da evolução das métricas de validação ao longo das épocas de treinamento e *fine-tuning*.  \n",
        "\n",
        "Os seguintes pontos foram analisados:  \n",
        "- **Acurácia de validação (val_accuracy):** mostra a taxa de acertos do modelo ao longo do treino.  \n",
        "- **Loss de validação (val_loss):** indica o quão bem o modelo está se ajustando aos dados de validação.  \n",
        "- **F1-score macro (val_f1_macro):** métrica mais robusta em cenários de classes parecidas e desbalanceadas, refletindo a média equilibrada entre precisão e revocação das classes.  \n",
        "\n",
        "Além da análise gráfica, foi feita a **avaliação final no conjunto de validação**, reportando a acurácia e o F1-score obtidos pelo modelo.  \n",
        "\n",
        "Por fim, o modelo final treinado foi salvo em disco no formato `.keras`, permitindo sua reutilização em inferências futuras sem necessidade de retreinamento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir históricos\n",
        "val_acc = history.history[\"val_accuracy\"] + history_finetune.history[\"val_accuracy\"]\n",
        "val_loss = history.history[\"val_loss\"] + history_finetune.history[\"val_loss\"]\n",
        "val_f1 = history.history.get(\"val_f1_macro\", []) + history_finetune.history.get(\"val_f1_macro\", [])\n",
        "epochs_range = range(1, len(val_acc) + 1)\n",
        "\n",
        "# Gráfico de Acurácia\n",
        "plt.plot(epochs_range, val_acc, label=\"Validação\")\n",
        "plt.title(\"Acurácia na validação (com Fine-tuning)\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Acurácia\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Gráfico de Loss\n",
        "plt.plot(epochs_range, val_loss, label=\"Validação\", color=\"orange\")\n",
        "plt.title(\"Loss na validação (com Fine-tuning)\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Val Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Gráfico de F1-score\n",
        "if val_f1:\n",
        "    plt.plot(epochs_range, val_f1, label=\"Validação\", color=\"green\")\n",
        "    plt.title(\"F1-score Macro na validação (com Fine-tuning)\")\n",
        "    plt.xlabel(\"Época\")\n",
        "    plt.ylabel(\"F1-score\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"⚠️ Nenhum valor de F1 encontrado nos logs.\")\n"
      ],
      "metadata": {
        "id": "8KtzIY2cc_CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3TZsbGYUUuC"
      },
      "source": [
        "## 11. Análise dos Resultados  \n",
        "\n",
        "A análise dos resultados mostrou que as categorias com **menor F1-score** foram justamente aquelas com maior similaridade visual:  \n",
        "\n",
        "- **Super Strato:** apresentou o pior desempenho. Isso era esperado, pois existem diferentes versões desse modelo e muitas delas se confundem com a **Stratocaster** em termos de formato e estética.  \n",
        "- **Semi Acústica:** também apresentou dificuldades, uma vez que alguns modelos possuem semelhanças visuais com a **SG** e a **Telecaster**, incluindo versões semi-acústicas dessas guitarras.  \n",
        "\n",
        "Por outro lado, categorias mais **únicas e facilmente distinguíveis** tiveram resultados muito bons, como é o caso da **Flying V**, que se destaca pelo formato característico.  \n",
        "\n",
        "De forma geral, o modelo apresentou:  \n",
        "- **Acurácia de validação próxima de 90%**  \n",
        "- **F1-score macro também próximo de 90%**  \n",
        "- **Validation loss em torno de 0.3**  \n",
        "\n",
        "Esses resultados indicam que o modelo possui um desempenho sólido e confiável, conseguindo diferenciar bem a maioria das classes, mesmo diante de categorias altamente semelhantes.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTxrynNZMpth"
      },
      "outputs": [],
      "source": [
        "# Avaliar modelo final\n",
        "loss, acc = model.evaluate(val_ds)\n",
        "print(f\"Acurácia final de validação: {acc:.2%}\")\n",
        "\n",
        "# Salvar modelo\n",
        "model.save(\"modelo_guitarras_mobilenetv2.keras\")\n",
        "\n",
        "# Previsões finais\n",
        "y_true, y_pred = [], []\n",
        "for images, labels in val_ds:\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "# Relatório por classe\n",
        "print(\"\\n📊 Relatório de Classificação:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Matriz de confusão\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"Matriz de Confusão\")\n",
        "plt.xlabel(\"Predito\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR8j1Ar8wJfj"
      },
      "source": [
        "### 12. Conclusão  \n",
        "\n",
        "O problema de classificação de guitarras apresentou desafios relevantes, principalmente devido à **similaridade visual entre determinados modelos**.  \n",
        "Exemplos claros foram:  \n",
        "- **Violão e Les Paul**, que possuem contornos semelhantes em algumas versões.  \n",
        "- **Stratocaster e Super Strato**, modelos muito próximos esteticamente.  \n",
        "- Inclusão de versões **Semi Acústicas** de Les Paul e Telecaster, que aumentaram a complexidade do problema.  \n",
        "\n",
        "Apesar dessas dificuldades, o modelo conseguiu atingir resultados consistentes, apresentando cerca de **90% de acurácia e F1-score macro** na validação, com um **val_loss em torno de 0.3**, o que indica um bom poder de generalização e um desempenho robusto para um dataset relativamente pequeno.  \n",
        "\n",
        "---\n",
        "\n",
        "## 12.1. Melhorias Propostas  \n",
        "\n",
        "Para melhorar ainda mais a performance e reduzir erros em classes semelhantes, podem ser exploradas as seguintes estratégias:  \n",
        "\n",
        "- **Aumento do dataset:** coletar mais imagens para equilibrar as classes e melhorar a generalização.  \n",
        "- **Pré-processamento das imagens:** remover fundos e centralizar a guitarra, diminuindo ruído visual.  \n",
        "- **Hierarquização das classes:** criar categorias maiores agrupando modelos semelhantes e treinar classificadores específicos para cada família.  \n",
        "  - Exemplo: *Família Les Paul* → Violão, Les Paul, SG e Semi Acústica.  \n",
        "  - Após a classificação inicial, um segundo modelo diferenciaria essas classes internas.  \n",
        "- **Técnicas avançadas de regularização:** aplicar *weight decay* ou *data augmentation* mais sofisticada (como *CutMix* ou *MixUp*) para reduzir overfitting.  \n",
        "- **Arquiteturas mais recentes:** testar modelos como **EfficientNetV2** ou **Vision Transformers (ViT)**, que podem trazer ganhos em cenários com classes visuais muito próximas.  \n",
        "\n",
        "---\n",
        "\n",
        "Em resumo, o modelo atual já se mostrou confiável e com ótimo desempenho para um **MVP**, mas há espaço para aprimoramentos que podem elevar sua robustez em aplicações reais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm7SDwWAdX9n"
      },
      "outputs": [],
      "source": [
        "# 📦 Importações\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# 📁 Upload da imagem\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "\n",
        "# 📌 Parâmetros\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# 🧠 Carrega o modelo\n",
        "model = load_model(\"modelo_guitarras_mobilenetv2.keras\")\n",
        "\n",
        "# 🔤 Carrega os nomes das classes (salvos em JSON na etapa de treino)\n",
        "with open(\"class_names.json\", \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "\n",
        "# 🖼️ Carrega a imagem preservando proporção\n",
        "img_raw = image.load_img(img_path)  # carrega original\n",
        "img_array = image.img_to_array(img_raw)\n",
        "\n",
        "# Redimensiona com padding (sem distorcer)\n",
        "img_array = tf.image.resize_with_pad(img_array, IMG_SIZE, IMG_SIZE).numpy()\n",
        "\n",
        "# Normaliza\n",
        "img_array = img_array / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# 📊 Faz a predição\n",
        "pred = model.predict(img_array)\n",
        "predicted_class = np.argmax(pred)\n",
        "\n",
        "# 🖨️ Resultado\n",
        "print(f\"Classe prevista: {class_names[predicted_class]}\")\n",
        "print(f\"Confiança: {pred[0][predicted_class]:.2%}\")\n",
        "\n",
        "# 🖼️ Mostrar imagem (a versão com padding)\n",
        "plt.imshow(tf.image.resize_with_pad(image.img_to_array(img_raw), IMG_SIZE, IMG_SIZE).numpy().astype(\"uint8\"))\n",
        "plt.axis('off')\n",
        "plt.title(f\"Predição: {class_names[predicted_class]} ({pred[0][predicted_class]:.2%})\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRA2K7VI5yrLoU7dQAdNFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}