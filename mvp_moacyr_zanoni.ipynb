{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoacyrZ/MVP---Data-Science/blob/main/mvp_moacyr_zanoni.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux4IR1RF2XEr"
      },
      "source": [
        "# Algoritmo de Identifica√ß√£o de Guitarras  \n",
        "\n",
        "## 1. Introdu√ß√£o  \n",
        "\n",
        "O objetivo deste projeto √© desenvolver um modelo de **deep learning** capaz de classificar imagens de guitarras el√©tricas e viol√µes em seus respectivos modelos: **Stratocaster, Les Paul, Telecaster, SG, Super Strato, Semi Ac√∫stica, Flying V e Viol√£o**. Trata-se, portanto, de um problema de **classifica√ß√£o multiclasse**, em que cada imagem deve ser associada corretamente a uma das oito categorias.  \n",
        "\n",
        "A aplica√ß√£o desse tipo de solu√ß√£o pode trazer benef√≠cios em diferentes contextos: **lojas de instrumentos musicais** que buscam organizar seu estoque automaticamente, **m√∫sicos** interessados em reconhecer modelos em cole√ß√µes pessoais, e **aplicativos de marketplace de instrumentos usados**, que poderiam oferecer identifica√ß√£o autom√°tica de an√∫ncios para melhorar a experi√™ncia do usu√°rio.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1.1 Considera√ß√µes  \n",
        "\n",
        "Como o projeto precisa ser reproduzido e avaliado pelos professores, foram adotados crit√©rios para garantir **viabilidade pr√°tica**:  \n",
        "- O **dataset** n√£o poderia ser excessivamente grande, evitando sobrecarga computacional.  \n",
        "- O processo de **treinamento e infer√™ncia** deveria ser executado em tempo razo√°vel, mas ainda assim garantindo **desempenho competitivo**.  \n",
        "- Foi escolhida uma arquitetura **pr√©-treinada e eficiente (MobileNetV2)**, que equilibra velocidade de execu√ß√£o com boa capacidade de generaliza√ß√£o.  \n",
        "\n",
        "---\n",
        "\n",
        "## 1.2 Desafios  \n",
        "\n",
        "O problema apresenta algumas complexidades espec√≠ficas:  \n",
        "- **Similaridade est√©tica** entre certos modelos (ex.: Stratocaster e Super Strato, Les Paul e Viol√£o).  \n",
        "- **Exist√™ncia de varia√ß√µes dentro da mesma categoria**, como vers√µes semi-ac√∫sticas de Les Paul e Telecaster.  \n",
        "- **Diversidade visual** em fotos reais, que podem ter fundos complexos, diferentes ilumina√ß√µes e √¢ngulos variados.  \n",
        "\n",
        "Esses fatores aumentam a dificuldade do modelo, tornando necess√°ria a aplica√ß√£o de t√©cnicas de **data augmentation, fine-tuning e avalia√ß√£o com m√©tricas robustas** como o **F1-score**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 2. Instala√ß√£o de Bibliotecas\n",
        "\n",
        "O primeiro bloco de c√≥digo do notebook √© respons√°vel pela **instala√ß√£o e importa√ß√£o das bibliotecas necess√°rias** para a execu√ß√£o do projeto. Essas bibliotecas incluem ferramentas para manipula√ß√£o de dados, processamento de imagens, constru√ß√£o e treinamento de modelos de deep learning, al√©m de fun√ß√µes auxiliares para visualiza√ß√£o gr√°fica e avalia√ß√£o de desempenho.  \n",
        "\n",
        "Em especial, destacam-se:  \n",
        "- **TensorFlow/Keras**: constru√ß√£o, treinamento e avalia√ß√£o da rede neural convolucional baseada na MobileNetV2.  \n",
        "- **NumPy**: manipula√ß√£o de arrays e opera√ß√µes matem√°ticas.  \n",
        "- **Matplotlib**: gera√ß√£o de gr√°ficos e visualiza√ß√µes dos resultados.  \n",
        "- **scikit-learn**: c√°lculo de m√©tricas de avalia√ß√£o adicionais, como o F1-score.  \n",
        "\n",
        "Esse passo garante que o ambiente de execu√ß√£o esteja devidamente configurado e com todas as depend√™ncias instaladas para rodar o MVP de forma reprodut√≠vel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE7OdwtXMq_7",
        "outputId": "d46c41b8-e33d-4ae6-8a4d-a6a4760826c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fcjx9Jyl6Ep-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import shutil\n",
        "import json\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import files\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Rescaling, RandomFlip, RandomRotation, RandomZoom,\n",
        "    Dense, GlobalAveragePooling2D, Dropout\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfEsU3XeNX9Z"
      },
      "source": [
        "## 3. Escolha do framework\n",
        "\n",
        "Ap√≥s analisar em detalhes as bibliotecas **Keras** e **PyTorch**, optei por utilizar o **Keras** como framework principal.  \n",
        "A decis√£o foi baseada em fatores como **simplicidade**, **produtividade** e **velocidade de prototipagem**, j√° que o objetivo do projeto √© testar diferentes t√©cnicas de deep learning de forma √°gil, com foco no aprendizado e na aplica√ß√£o pr√°tica.  \n",
        "Embora o PyTorch ofere√ßa maior flexibilidade e controle fino sobre o modelo, o Keras permite atingir resultados competitivos com menos complexidade de c√≥digo, o que foi determinante para este MVP.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Escolha da arquitetura\n",
        "\n",
        "A escolha da arquitetura foi guiada pelo problema proposto: **classifica√ß√£o multiclasse de guitarras el√©tricas e viol√£o**.  \n",
        "Inicialmente, defini que seria utilizado **Deep Learning**, pois al√©m de ser uma √°rea de interesse pessoal, o projeto tamb√©m contribui para o fortalecimento do portf√≥lio acad√™mico e profissional.  \n",
        "Foram avaliadas algumas arquiteturas pr√©-treinadas dispon√≠veis no Keras Applications, considerando principalmente o **equil√≠brio entre custo computacional e desempenho**.\n",
        "\n",
        "### 4.1 MobileNetV2\n",
        "Selecionada por ser uma arquitetura **leve e eficiente**, projetada para rodar em dispositivos com recursos limitados.  \n",
        "Por sua rapidez e baixo consumo de mem√≥ria, encaixa-se perfeitamente nos requisitos do projeto, mantendo boa capacidade de generaliza√ß√£o.\n",
        "\n",
        "### 4.2 EfficientNetB0\n",
        "Considerada por seu **excelente trade-off entre acur√°cia e efici√™ncia computacional**.  \n",
        "√â uma arquitetura moderna, baseada em um escalonamento balanceado de profundidade, largura e resolu√ß√£o, oferecendo bom desempenho mesmo com custo computacional reduzido.\n",
        "\n",
        "### 4.3 InceptionV3\n",
        "Inclu√≠da para diversifica√ß√£o dos testes, de modo a comparar arquiteturas leves (MobileNetV2 e EfficientNetB0) com uma op√ß√£o **mais robusta**.  \n",
        "Apesar de mais pesada, o InceptionV3 oferece um alto poder de extra√ß√£o de caracter√≠sticas e foi utilizado como refer√™ncia para avaliar poss√≠veis ganhos de desempenho em rela√ß√£o √†s arquiteturas otimizadas para efici√™ncia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXciL-hhVdYU"
      },
      "source": [
        "## 5. Constru√ß√£o e Evolu√ß√£o do Dataset  \n",
        "\n",
        "O dataset utilizado neste projeto foi desenvolvido a partir de diferentes fontes e passou por diversas vers√µes at√© atingir um ponto considerado adequado para o treinamento do modelo.  \n",
        "A base inicial foi obtida no reposit√≥rio p√∫blico [Guitar Image Classification Dataset](https://images.cv/dataset/guitar-image-classification-dataset), posteriormente ampliada e refinada com imagens coletadas via *web scraping*.  \n",
        "\n",
        "As vers√µes foram evoluindo da seguinte forma:  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.1 Dataset V1  \n",
        "Utiliza√ß√£o direta do dataset original do link mencionado.  \n",
        "As imagens estavam organizadas em pastas correspondentes aos diferentes modelos de guitarras.  \n",
        "**Resultado:** o desempenho do modelo foi insatisfat√≥rio, evidenciando a necessidade de ampliar a base de dados.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.2 Dataset V2  \n",
        "Amplia√ß√£o do dataset com imagens coletadas por *web scraping*, cobrindo diferentes modelos de guitarras.  \n",
        "**Resultado:** houve melhora em rela√ß√£o ao V1, mas ainda insuficiente para um bom n√≠vel de generaliza√ß√£o.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.3 Dataset V3  \n",
        "Refinamento da curadoria das imagens, com remo√ß√£o de:  \n",
        "- Fotos contendo mais de uma guitarra.  \n",
        "- Imagens de guitarras fotografadas de lado ou de costas.  \n",
        "- Imagens de baixa qualidade ou em que a guitarra n√£o aparecia de forma clara.  \n",
        "**Resultado:** melhoria significativa nos resultados do modelo, mostrando a import√¢ncia da qualidade dos dados.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.4 Dataset V4  \n",
        "Aumento direcionado do n√∫mero de imagens em classes mais complexas, onde o modelo apresentava maior dificuldade, como:  \n",
        "- **Stratocaster vs. Super Strato**  \n",
        "- **Les Paul vs. Semi Ac√∫stica**  \n",
        "**Resultado:** maior equil√≠brio entre classes e melhor desempenho nas categorias mais semelhantes entre si.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5.5 Dataset Final  \n",
        "Com o dataset consolidado, foi realizado o **redimensionamento das imagens**, de modo a reduzir o espa√ßo de armazenamento e padronizar a entrada do modelo (224x224 pixels), garantindo consist√™ncia durante o treinamento e a valida√ß√£o.  \n",
        "**Resultado:** dataset mais balanceado, limpo e adequado para experimenta√ß√£o com diferentes arquiteturas de deep learning.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20azK6s48Qkc",
        "outputId": "43d60fa9-1e0a-4f23-bc97-894b6ca8b188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extra√≠do em: dataset\n",
            "Subpastas: ['sg', 'violao', 'semi_acustica', 'flying_v', 'telecaster', 'super_strato', 'les_paul', 'stratocaster']\n",
            "sg: 157 imagens\n",
            "violao: 140 imagens\n",
            "semi_acustica: 147 imagens\n",
            "flying_v: 139 imagens\n",
            "telecaster: 129 imagens\n",
            "super_strato: 137 imagens\n",
            "les_paul: 168 imagens\n",
            "stratocaster: 153 imagens\n"
          ]
        }
      ],
      "source": [
        "# URL do dataset zipado no GitHub (raw link!)\n",
        "url = \"https://raw.githubusercontent.com/MoacyrZ/MVP---Data-Science/main/dataset.zip\"\n",
        "zip_name = \"dataset.zip\"\n",
        "extract_dir = \"dataset\"\n",
        "\n",
        "# Se j√° existir dataset antigo, apagar\n",
        "if os.path.exists(extract_dir):\n",
        "    shutil.rmtree(extract_dir)\n",
        "\n",
        "if os.path.exists(zip_name):\n",
        "    os.remove(zip_name)\n",
        "\n",
        "# Baixar sempre a vers√£o mais recente do GitHub\n",
        "r = requests.get(url)\n",
        "with open(zip_name, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "# Extrair\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Ajustar automaticamente se houver subpasta √∫nica (ex: \"1\")\n",
        "subpastas = [p for p in os.listdir(extract_dir) if os.path.isdir(os.path.join(extract_dir, p))]\n",
        "if len(subpastas) == 1:\n",
        "    DATASET_DIR = os.path.join(extract_dir, subpastas[0])\n",
        "else:\n",
        "    DATASET_DIR = extract_dir\n",
        "\n",
        "print(f\"Dataset extra√≠do em: {DATASET_DIR}\")\n",
        "print(\"Subpastas:\", os.listdir(DATASET_DIR))\n",
        "\n",
        "# Mostrar quantidade de imagens por pasta\n",
        "for pasta in os.listdir(DATASET_DIR):\n",
        "    caminho = os.path.join(DATASET_DIR, pasta)\n",
        "    if os.path.isdir(caminho):\n",
        "        print(f\"{pasta}: {len(os.listdir(caminho))} imagens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StLbDAEAXRT3"
      },
      "source": [
        "## 6. Analisando o Dataset  \n",
        "\n",
        "Nesta etapa foi realizada uma contagem do n√∫mero de imagens em cada categoria, a fim de avaliar se o dataset apresenta **balanceamento adequado** entre as classes.  \n",
        "A an√°lise da distribui√ß√£o √© fundamental para entender poss√≠veis **desequil√≠brios**, que podem impactar o treinamento do modelo e gerar vi√©s em algumas classes.  \n",
        "\n",
        "Al√©m da contagem num√©rica, tamb√©m foi gerado um gr√°fico de barras para facilitar a visualiza√ß√£o das diferen√ßas no n√∫mero de exemplos por categoria.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WT7cou2PKL6S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad1d81c-bfca-455e-da54-e329a201bb94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1170 files belonging to 8 classes.\n",
            "Using 936 files for training.\n",
            "Found 1170 files belonging to 8 classes.\n",
            "Using 234 files for validation.\n",
            "N√∫mero de classes: 8\n",
            "Classes: ['flying_v', 'les_paul', 'semi_acustica', 'sg', 'stratocaster', 'super_strato', 'telecaster', 'violao']\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "\n",
        "# Dataset de treino\n",
        "train_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATASET_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=SEED,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "# Dataset de valida√ß√£o\n",
        "val_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    DATASET_DIR,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=SEED,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    label_mode='categorical',\n",
        "    crop_to_aspect_ratio=True\n",
        ")\n",
        "\n",
        "# Classes\n",
        "class_names = train_ds_raw.class_names\n",
        "num_classes = len(class_names)\n",
        "print(f\"N√∫mero de classes: {num_classes}\")\n",
        "print(f\"Classes: {class_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9BkYLE8ZZBn"
      },
      "source": [
        "## 7. Transforma√ß√£o das Imagens e Data Augmentation  \n",
        "\n",
        "Todas as imagens foram padronizadas para o tamanho de **224x224 pixels**, garantindo compatibilidade com a arquitetura escolhida (MobileNetV2).  \n",
        "Na primeira tentativa, o redimensionamento foi feito de forma direta, o que causou **distor√ß√£o nas propor√ß√µes** das guitarras.  \n",
        "Ap√≥s identificar esse problema, a abordagem foi modificada utilizando **`resize_with_pad`**, que adiciona tarjas pretas quando necess√°rio, mantendo as propor√ß√µes originais da imagem.  \n",
        "Esse ajuste resultou em uma melhoria no desempenho do modelo.  \n",
        "\n",
        "---\n",
        "\n",
        "### 7.1 Separa√ß√£o do Dataset de Treino  \n",
        "Foi realizada a divis√£o cl√°ssica **80% para treino** e **20% para valida√ß√£o**.  \n",
        "Apesar do dataset relativamente pequeno, optou-se por manter essa propor√ß√£o para garantir volume suficiente de amostras para treino sem comprometer a valida√ß√£o.  \n",
        "\n",
        "---\n",
        "\n",
        "### 7.2 Separa√ß√£o do Dataset de Valida√ß√£o  \n",
        "Os **20% restantes** foram destinados exclusivamente √† valida√ß√£o, garantindo que o modelo fosse avaliado com dados nunca vistos durante o treino.  \n",
        "\n",
        "---\n",
        "\n",
        "### 7.3 Data Augmentation  \n",
        "Como o dataset possui em m√©dia **150 imagens por categoria**, a t√©cnica de **Data Augmentation** foi fundamental para aumentar a variabilidade dos exemplos de treino.  \n",
        "\n",
        "- **Treino**: aplicadas as seguintes transforma√ß√µes:  \n",
        "  - Normaliza√ß√£o (escala 0‚Äì1).  \n",
        "  - **Random Flip** (espelhamento horizontal).  \n",
        "  - **Random Rotation** (rota√ß√µes leves).  \n",
        "  - **Random Zoom** (amplia√ß√µes e redu√ß√µes parciais).  \n",
        "\n",
        "- **Valida√ß√£o**: aplicada apenas a **normaliza√ß√£o**, evitando *data leakage* e assegurando avalia√ß√£o justa do desempenho do modelo.  \n",
        "\n",
        "Essa estrat√©gia contribuiu para uma **melhor generaliza√ß√£o**, reduzindo o risco de *overfitting* e melhorando a robustez do modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8WeU3bGLULQ"
      },
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "data_augmentation = Sequential([\n",
        "    Rescaling(1./255),\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.1),\n",
        "    RandomZoom(0.1)\n",
        "])\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds_raw.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = val_ds_raw.map(lambda x, y: (Rescaling(1./255)(x), y))\n",
        "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Salvar nomes das classes\n",
        "with open(\"class_names.json\", \"w\") as f:\n",
        "    json.dump(class_names, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L11VhNdYiVzI"
      },
      "source": [
        "## 8. Experimentos  \n",
        "\n",
        "O primeiro experimento consistiu em rodar a arquitetura **MobileNetV2** com a primeira vers√£o do dataset, sem t√©cnicas de pr√©-processamento ou refinamento. O resultado obtido foi de aproximadamente **12% de acur√°cia**, o que equivale a uma classifica√ß√£o aleat√≥ria.  \n",
        "\n",
        "Na segunda vers√£o do dataset, com mais imagens e ap√≥s 30 √©pocas de treino, o desempenho subiu para aproximadamente **50% de acur√°cia**, mostrando que o modelo j√° n√£o fazia infer√™ncias totalmente aleat√≥rias.  \n",
        "\n",
        "A seguir, foram testadas tr√™s arquiteturas diferentes:  \n",
        "- **MobileNetV2**  \n",
        "- **EfficientNetB0**  \n",
        "- **InceptionV3**  \n",
        "\n",
        "Os resultados indicaram que **MobileNetV2** e **InceptionV3** obtiveram desempenhos razo√°veis (em torno de 50%), enquanto o **EfficientNetB0** ficou em apenas 15%. Dada a simplicidade, efici√™ncia e compatibilidade com os recursos dispon√≠veis, foi escolhida a **MobileNetV2** para dar continuidade ao projeto.  \n",
        "\n",
        "Com a arquitetura definida, foram aplicadas melhorias no dataset e na otimiza√ß√£o com o **Adam**, elevando os resultados para aproximadamente **75% de acur√°cia**.  \n",
        "\n",
        "---\n",
        "\n",
        "## 9. Pipeline de Treinamento e Fine-Tuning  \n",
        "\n",
        "O pipeline de treinamento foi estruturado em diferentes etapas, utilizando **transfer learning** a partir de uma base pr√©-treinada no **ImageNet**. Essa abordagem permite aproveitar o conhecimento pr√©vio da rede em detec√ß√£o de padr√µes, formas e texturas, acelerando o processo de aprendizagem.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.1 Transfer Learning  \n",
        "Foi substitu√≠da a ‚Äúcabe√ßa‚Äù do modelo pr√©-treinado, adaptando a rede para classificar as **8 categorias de guitarras** do projeto.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.2 Congelamento da Base  \n",
        "Inicialmente, a base do modelo pr√©-treinado foi **congelada**, de modo a preservar o conhecimento adquirido no ImageNet e evitar que fosse sobrescrito logo nas primeiras √©pocas.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.3 Escolha da M√©trica  \n",
        "Como se trata de um problema de **classifica√ß√£o multiclasse**, as m√©tricas monitoradas foram:  \n",
        "- **Acur√°cia**, para medir a taxa de acertos globais.  \n",
        "- **F1-score (macro)**, para capturar o desempenho m√©dio entre as classes e evitar que categorias mais frequentes dominassem a avalia√ß√£o.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.4 Otimizador  \n",
        "Foram estudadas alternativas de otimizadores, mas o **Adam** apresentou melhor equil√≠brio entre velocidade de converg√™ncia e desempenho final. Por ser adaptativo e eficiente, foi mantido como padr√£o nos experimentos.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.5 Lidando com Overfitting  \n",
        "Durante os experimentos iniciais, o modelo apresentou sinais de **overfitting**. Para lidar com isso, foram adotadas as seguintes estrat√©gias:  \n",
        "- Ajuste da taxa de **Dropout**, reduzindo a complexidade da rede.  \n",
        "- Uso do **EarlyStopping**, interrompendo o treino quando n√£o havia mais ganhos significativos.  \n",
        "\n",
        "---\n",
        "\n",
        "### 9.6 Fine-Tuning  \n",
        "Na etapa final, a base pr√©-treinada foi **parcialmente descongelada**, liberando apenas as √∫ltimas camadas para ajuste fino. Esse processo foi conduzido com uma **taxa de aprendizado reduzida**, permitindo que o modelo refinasse os pesos em fun√ß√£o das guitarras, sem comprometer o conhecimento geral adquirido com o ImageNet.  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MetricsCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, val_ds):\n",
        "        super().__init__()\n",
        "        self.val_ds = val_ds\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        y_true, y_pred = [], []\n",
        "        for x, y in self.val_ds:\n",
        "            preds = self.model.predict(x, verbose=0)\n",
        "            y_true.extend(np.argmax(y.numpy(), axis=1))\n",
        "            y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1} - Val Accuracy: {acc:.4f} - Val F1 (macro): {f1:.4f}\")\n",
        "        logs[\"val_accuracy_sklearn\"] = acc\n",
        "        logs[\"val_f1_macro\"] = f1\n"
      ],
      "metadata": {
        "id": "q0MDAqRLgPSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyqG2p7IMVD9"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 12\n",
        "INITIAL_EPOCH = 4\n",
        "LEARNING_RATE = 0.001\n",
        "FINE_TUNE_EPOCHS = 10\n",
        "\n",
        "UNITS_0, DROPOUT_0 = 128, 0.2\n",
        "UNITS_1, DROPOUT_1 = 448, 0.3\n",
        "\n",
        "# Base pr√©-treinada\n",
        "base_model = MobileNetV2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "base_model.trainable = False  # congelada inicialmente\n",
        "\n",
        "# Cabe√ßa customizada\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(UNITS_0, activation=\"relu\")(x)\n",
        "x = Dropout(DROPOUT_0)(x)\n",
        "x = Dense(UNITS_1, activation=\"relu\")(x)\n",
        "x = Dropout(DROPOUT_1)(x)\n",
        "output = Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compila√ß√£o\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "metrics_callback = MetricsCallback(val_ds)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[early_stop, metrics_callback]\n",
        ")\n"
      ],
      "metadata": {
        "id": "tR6w7Gkygkpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descongela parte da base\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "for layer in base_model.layers[:-50]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompilar\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-5),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "early_stop_ft = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
        "metrics_callback_ft = MetricsCallback(val_ds)\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=FINE_TUNE_EPOCHS,\n",
        "    callbacks=[early_stop_ft, metrics_callback_ft]\n",
        ")\n"
      ],
      "metadata": {
        "id": "szurU6pOgyov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQbdEMqy3l7h"
      },
      "source": [
        "## 10. Avalia√ß√£o e Gr√°ficos  \n",
        "\n",
        "Para avaliar o desempenho do modelo, foram gerados gr√°ficos da evolu√ß√£o das m√©tricas de valida√ß√£o ao longo das √©pocas de treinamento e *fine-tuning*.  \n",
        "\n",
        "Os seguintes pontos foram analisados:  \n",
        "- **Acur√°cia de valida√ß√£o (val_accuracy):** mostra a taxa de acertos do modelo ao longo do treino.  \n",
        "- **Loss de valida√ß√£o (val_loss):** indica o qu√£o bem o modelo est√° se ajustando aos dados de valida√ß√£o.  \n",
        "- **F1-score macro (val_f1_macro):** m√©trica mais robusta em cen√°rios de classes parecidas e desbalanceadas, refletindo a m√©dia equilibrada entre precis√£o e revoca√ß√£o das classes.  \n",
        "\n",
        "Al√©m da an√°lise gr√°fica, foi feita a **avalia√ß√£o final no conjunto de valida√ß√£o**, reportando a acur√°cia e o F1-score obtidos pelo modelo.  \n",
        "\n",
        "Por fim, o modelo final treinado foi salvo em disco no formato `.keras`, permitindo sua reutiliza√ß√£o em infer√™ncias futuras sem necessidade de retreinamento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir hist√≥ricos\n",
        "val_acc = history.history[\"val_accuracy\"] + history_finetune.history[\"val_accuracy\"]\n",
        "val_loss = history.history[\"val_loss\"] + history_finetune.history[\"val_loss\"]\n",
        "val_f1 = history.history.get(\"val_f1_macro\", []) + history_finetune.history.get(\"val_f1_macro\", [])\n",
        "epochs_range = range(1, len(val_acc) + 1)\n",
        "\n",
        "# Gr√°fico de Acur√°cia\n",
        "plt.plot(epochs_range, val_acc, label=\"Valida√ß√£o\")\n",
        "plt.title(\"Acur√°cia na valida√ß√£o (com Fine-tuning)\")\n",
        "plt.xlabel(\"√âpoca\")\n",
        "plt.ylabel(\"Acur√°cia\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Gr√°fico de Loss\n",
        "plt.plot(epochs_range, val_loss, label=\"Valida√ß√£o\", color=\"orange\")\n",
        "plt.title(\"Loss na valida√ß√£o (com Fine-tuning)\")\n",
        "plt.xlabel(\"√âpoca\")\n",
        "plt.ylabel(\"Val Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Gr√°fico de F1-score\n",
        "if val_f1:\n",
        "    plt.plot(epochs_range, val_f1, label=\"Valida√ß√£o\", color=\"green\")\n",
        "    plt.title(\"F1-score Macro na valida√ß√£o (com Fine-tuning)\")\n",
        "    plt.xlabel(\"√âpoca\")\n",
        "    plt.ylabel(\"F1-score\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Nenhum valor de F1 encontrado nos logs.\")\n"
      ],
      "metadata": {
        "id": "8KtzIY2cc_CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3TZsbGYUUuC"
      },
      "source": [
        "## 11. An√°lise dos Resultados  \n",
        "\n",
        "A an√°lise dos resultados mostrou que as categorias com **menor F1-score** foram justamente aquelas com maior similaridade visual:  \n",
        "\n",
        "- **Super Strato:** apresentou o pior desempenho. Isso era esperado, pois existem diferentes vers√µes desse modelo e muitas delas se confundem com a **Stratocaster** em termos de formato e est√©tica.  \n",
        "- **Semi Ac√∫stica:** tamb√©m apresentou dificuldades, uma vez que alguns modelos possuem semelhan√ßas visuais com a **SG** e a **Telecaster**, incluindo vers√µes semi-ac√∫sticas dessas guitarras.  \n",
        "\n",
        "Por outro lado, categorias mais **√∫nicas e facilmente distingu√≠veis** tiveram resultados muito bons, como √© o caso da **Flying V**, que se destaca pelo formato caracter√≠stico.  \n",
        "\n",
        "De forma geral, o modelo apresentou:  \n",
        "- **Acur√°cia de valida√ß√£o pr√≥xima de 90%**  \n",
        "- **F1-score macro tamb√©m pr√≥ximo de 90%**  \n",
        "- **Validation loss em torno de 0.3**  \n",
        "\n",
        "Esses resultados indicam que o modelo possui um desempenho s√≥lido e confi√°vel, conseguindo diferenciar bem a maioria das classes, mesmo diante de categorias altamente semelhantes.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTxrynNZMpth"
      },
      "outputs": [],
      "source": [
        "# Avaliar modelo final\n",
        "loss, acc = model.evaluate(val_ds)\n",
        "print(f\"Acur√°cia final de valida√ß√£o: {acc:.2%}\")\n",
        "\n",
        "# Salvar modelo\n",
        "model.save(\"modelo_guitarras_mobilenetv2.keras\")\n",
        "\n",
        "# Previs√µes finais\n",
        "y_true, y_pred = [], []\n",
        "for images, labels in val_ds:\n",
        "    preds = model.predict(images)\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "# Relat√≥rio por classe\n",
        "print(\"\\nüìä Relat√≥rio de Classifica√ß√£o:\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "# Matriz de confus√£o\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"Matriz de Confus√£o\")\n",
        "plt.xlabel(\"Predito\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR8j1Ar8wJfj"
      },
      "source": [
        "### 12. Conclus√£o  \n",
        "\n",
        "O problema de classifica√ß√£o de guitarras apresentou desafios relevantes, principalmente devido √† **similaridade visual entre determinados modelos**.  \n",
        "Exemplos claros foram:  \n",
        "- **Viol√£o e Les Paul**, que possuem contornos semelhantes em algumas vers√µes.  \n",
        "- **Stratocaster e Super Strato**, modelos muito pr√≥ximos esteticamente.  \n",
        "- Inclus√£o de vers√µes **Semi Ac√∫sticas** de Les Paul e Telecaster, que aumentaram a complexidade do problema.  \n",
        "\n",
        "Apesar dessas dificuldades, o modelo conseguiu atingir resultados consistentes, apresentando cerca de **90% de acur√°cia e F1-score macro** na valida√ß√£o, com um **val_loss em torno de 0.3**, o que indica um bom poder de generaliza√ß√£o e um desempenho robusto para um dataset relativamente pequeno.  \n",
        "\n",
        "---\n",
        "\n",
        "## 12.1. Melhorias Propostas  \n",
        "\n",
        "Para melhorar ainda mais a performance e reduzir erros em classes semelhantes, podem ser exploradas as seguintes estrat√©gias:  \n",
        "\n",
        "- **Aumento do dataset:** coletar mais imagens para equilibrar as classes e melhorar a generaliza√ß√£o.  \n",
        "- **Pr√©-processamento das imagens:** remover fundos e centralizar a guitarra, diminuindo ru√≠do visual.  \n",
        "- **Hierarquiza√ß√£o das classes:** criar categorias maiores agrupando modelos semelhantes e treinar classificadores espec√≠ficos para cada fam√≠lia.  \n",
        "  - Exemplo: *Fam√≠lia Les Paul* ‚Üí Viol√£o, Les Paul, SG e Semi Ac√∫stica.  \n",
        "  - Ap√≥s a classifica√ß√£o inicial, um segundo modelo diferenciaria essas classes internas.  \n",
        "- **T√©cnicas avan√ßadas de regulariza√ß√£o:** aplicar *weight decay* ou *data augmentation* mais sofisticada (como *CutMix* ou *MixUp*) para reduzir overfitting.  \n",
        "- **Arquiteturas mais recentes:** testar modelos como **EfficientNetV2** ou **Vision Transformers (ViT)**, que podem trazer ganhos em cen√°rios com classes visuais muito pr√≥ximas.  \n",
        "\n",
        "---\n",
        "\n",
        "Em resumo, o modelo atual j√° se mostrou confi√°vel e com √≥timo desempenho para um **MVP**, mas h√° espa√ßo para aprimoramentos que podem elevar sua robustez em aplica√ß√µes reais.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm7SDwWAdX9n"
      },
      "outputs": [],
      "source": [
        "# üì¶ Importa√ß√µes\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from google.colab import files\n",
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "# üìÅ Upload da imagem\n",
        "uploaded = files.upload()\n",
        "img_path = list(uploaded.keys())[0]\n",
        "\n",
        "# üìå Par√¢metros\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# üß† Carrega o modelo\n",
        "model = load_model(\"modelo_guitarras_mobilenetv2.keras\")\n",
        "\n",
        "# üî§ Carrega os nomes das classes (salvos em JSON na etapa de treino)\n",
        "with open(\"class_names.json\", \"r\") as f:\n",
        "    class_names = json.load(f)\n",
        "\n",
        "# üñºÔ∏è Carrega a imagem preservando propor√ß√£o\n",
        "img_raw = image.load_img(img_path)  # carrega original\n",
        "img_array = image.img_to_array(img_raw)\n",
        "\n",
        "# Redimensiona com padding (sem distorcer)\n",
        "img_array = tf.image.resize_with_pad(img_array, IMG_SIZE, IMG_SIZE).numpy()\n",
        "\n",
        "# Normaliza\n",
        "img_array = img_array / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# üìä Faz a predi√ß√£o\n",
        "pred = model.predict(img_array)\n",
        "predicted_class = np.argmax(pred)\n",
        "\n",
        "# üñ®Ô∏è Resultado\n",
        "print(f\"Classe prevista: {class_names[predicted_class]}\")\n",
        "print(f\"Confian√ßa: {pred[0][predicted_class]:.2%}\")\n",
        "\n",
        "# üñºÔ∏è Mostrar imagem (a vers√£o com padding)\n",
        "plt.imshow(tf.image.resize_with_pad(image.img_to_array(img_raw), IMG_SIZE, IMG_SIZE).numpy().astype(\"uint8\"))\n",
        "plt.axis('off')\n",
        "plt.title(f\"Predi√ß√£o: {class_names[predicted_class]} ({pred[0][predicted_class]:.2%})\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRA2K7VI5yrLoU7dQAdNFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}